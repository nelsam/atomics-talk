<!DOCTYPE html>
<html>
<head>
<title>Communicate by Sharing Memory</title>
<meta charset="utf-8">
<style>

@import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
@import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
@import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

body {
    font-family: 'Droid Serif';
}
h1, h2, h3 {
    font-family: 'Yanone Kaffeesatz';
    font-weight: normal;
}
.remark-code, .remark-inline-code {
    font-family: 'Ubuntu Mono';
}
table {
    margin: 25px;
}
table td {
    padding: 5px;
}
table td i {
    color: red;
}
span.strike {
    text-decoration: line-through;
}
</style>
</head>
<body>
<textarea id="source">

class: center, middle

# Communicate by Sharing Memory
## &#x269b;&#xfe0e; Atomics and Mutexes in Go

---

class: center, middle

# Me

Samuel Nelson<br>
I work at Digital Ocean!<br>
github.com/nelsam
github.com/go-gorp

Credit for the original slide deck<br>
goes to Jason Keene, senior engineer<br>
at Pivotal Software.

---

name: synchronizing

## Synchronization

Synchronization is serializing access to shared resources between
multiple execution contexts (threads, OS processes, coroutines, etc).

Our options for synchronization in go:

 - `chan`
 - `sync`
 - `atomic`
 - use cgo to call into all sorts of things
 - golang asm using ISA specific instructions

Let's investigate these options.

---

name: synchronizing

## Synchronization

Synchronization is serializing access to shared resources between
multiple execution contexts (threads, OS processes, coroutines, etc).

Our options for synchronization in go:

 - `chan`
 - `sync`
 - `atomic`
 - <span class="strike">use cgo to call into all sorts of things</span>
 - <span class="strike">golang asm using ISA specific instructions</span>

Let's investigate <i>some of</i> these options.

---

## Reminder: channel primitive

- `chan someType`

---

## Package overview: sync

`sync` provides logic to claim and release locks.

For the purposes of this talk, we care about:

 - Mutex
 - RWMutex
 - Cond

---

## Package overview: atomic

Atomic operations appear to the rest of the system to occur instantaneously.

Atomic operations fall into the [following categories](https://golang.org/pkg/sync/atomic/#pkg-index):

 - Load/Store
 - Add
 - Swap
 - Compare and Swap (CAS)

Gotchas:

 - Byte alignment on 386_32 and ARM.
 - 64bit values on very old 32bit architectures.

---

A naive implementation of a concurrent counter:

```go
var value int

func loop(count int) {
    fmt.Printf("Started looping: %d\n", count)
    amount := 1
    if count < 0 {
        count = -count
        amount = -1
    }

    for i := 0; i < count; i++ {
        value += amount
    }
}

func main() {
    var wg sync.WaitGroup
    wg.Add(1)
    go func() {
        loop(200000000)
        wg.Done()
    }()
    loop(-100000000)
    wg.Wait()
    fmt.Printf("Resulting value: %d\n", value)
}
```

---

## Naive Result

```
$ time ./naive
Started looping: 10000000
Started looping: 20000000
Resulting value: 20238002

real    0m0.067s
user    0m0.087s
sys     0m0.007s
```

Very fast but not correct, we must synchronize!

But why was the result not correct?

 - main memory access is linearized (things happen in an order)
    - Read value for g1
    - Read value for g2
    - Write value for g1
    - Write value for g2
 - often you are not even reading/writing to main memory but L1/2/3

---

Let's use channels:

```diff
+var change chan int

@@ -16,18 +19,31 @@
    for i := 0; i < count; i++ {
-       value += amount
+       change <- amount
    }
 }

 func main() {
    var wg sync.WaitGroup
+   var writerWg sync.WaitGroup
+   change = make(chan int, 100)
    wg.Add(1)
+   writerWg.Add(1)
+   go func() {
+       for amount := range change {
+           value += amount
+       }
+       writerWg.Done()
+   }()
    go func() {
        loop(200000000)
        wg.Done()
    }()
    loop(-100000000)
    wg.Wait()
+   close(change)
+   writerWg.Wait()
```

---

## Channel Result

```
$ time ./channel
Started looping: -100000000
Started looping: 200000000
Resulting value: 100000000

real    0m32.919s
user    0m36.035s
sys     0m3.274s
```

Correct result but super slow.

---

Let's add a tight mutex around the writes:

```diff
+var mu    sync.Mutex

 func loop(count int) {
    fmt.Printf("Started looping: %d\n", count)
@@ -16,7 +19,9 @@
    }

    for i := 0; i < count; i++ {
+       mu.Lock()
        value += amount
+       mu.Unlock()
    }
 }
```

---

## Mutex Result

```
Started looping: -100000000
Started looping: 200000000
Resulting value: 100000000

real    0m21.744s
user    0m38.379s
sys     0m0.284s
```

Still very slow. But why?

 - Lock overhead (we are doing 300,000,000 Lock() and Unlock() calls).

---

Let's use atomic addition:

```go
-var value int
+var value int64

 func loop(count int) {
    fmt.Printf("Started looping: %d\n", count)
-   amount := 1
+   amount := int64(1)
    if count < 0 {
        count = -count
        amount = -1
    }

    for i := 0; i < count; i++ {
-       value += amount
+       atomic.AddInt64(&value, amount)
    }
 }
```

---

## Atomic Result

```
$ time ./atomic
Started looping: -100000000
Started looping: 200000000
Resulting value: 100000000

real    0m7.232s
user    0m13.445s
sys     0m0.027s
```

Not quite as slow, but still not great.

---

Let's just do these mutations serially:

```go
 func main() {
-   var wg sync.WaitGroup
-   wg.Add(1)
-   go func() {
-       loop(200000000)
-       wg.Done()
-   }()
+   loop(200000000)
    loop(-100000000)
-   wg.Wait()
    fmt.Printf("Resulting value: %d\n", value)
 }
```

---

## Serial Results

```
$ time ./serial
Started looping: 200000000
Started looping: -100000000
Resulting value: 100000000

real    0m0.733s
user    0m0.708s
sys     0m0.011s
```

---

Let's look at some more benchmarks:

| **GOMAXPROCS**<br>**GOROUTINES** |  <br>2 | 1<br>4 |  <br>8 |  <br>2 | 2<br>4 |  <br>8 |
|---------------------------------:|:------:|:------:|:------:|:------:|:------:|:------:|
|                        **Naive** |  0.556 |  0.566 |  0.551 |  <i>0.387</i> |  <i>0.327</i> |  <i>0.307</i> |
|                      **Channel** | 19.728 | 19.491 | 19.584 | 20.987 | 22.675 | 22.724 |
|                        **Mutex** |  5.837 |  7.688 |  7.453 |  8.174 | 25.794 | 26.316 |
|                       **Atomic** |  2.313 |  2.300 |  2.297 |  4.203 |  4.905 |  4.447 |
|                       **Serial** |  0.577 |  0.575 |  0.570 |  0.553 |  0.577 |  0.557 |


| **GOMAXPROCS**<br>**GOROUTINES** |  <br>2 | 4<br>4 |  <br>8 |  <br>2 | 8<br>4 |  <br>8 |
|---------------------------------:|:------:|:------:|:------:|:------:|:------:|:------:|
|                        **Naive** |  <i>0.376</i> |  <i>0.234</i> |  <i>0.255</i> |  <i>0.388</i> |  <i>0.253</i> |  <i>0.319</i> |
|                      **Channel** | 22.046 | 23.308 | 23.257 | 22.685 | 24.773 | 26.175 |
|                        **Mutex** |  8.260 | 25.066 | 26.887 |  8.430 | 25.132 | 23.923 |
|                       **Atomic** |  4.190 |  4.787 |  5.112 |  4.094 |  4.606 |  4.159 |
|                       **Serial** |  0.574 |  0.567 |  0.575 |  0.579 |  0.563 |  0.572 |

_Ran on a 4 core single socket x86_64._

---

Let's look at some more robust benchmarks:

<img src="/chart.png" style="margin:20px 0 18px 0;" />

_Ran on a 4 core single socket x86_64._

---

# The cost of synchronization

 - Clearly a contrived example, but locking is expensive
 - This is, in essence, "parallel" but not "concurrent"
   - Executing on multiple cores
   - Not designed to spread the load
 - Channels are slow

---

## Since channels are so awful, let's just replace them with atomics!

```go
type lock uint32

func (l *lock) Lock() {
    for !atomic.CompareAndSwapUint32(&c.locked, 0, 1) {
        runtime.Gosched()
    }
}

func (l *lock) Unlock() {
    atomic.StoreUint32(&c.locked, 0)
}
```

---

```go
type Channel struct {
    l        lock
    elements []float64
    closed   uint32
}

func (c *Channel) Send(v []byte) {
    for !c.trySend(v) {
        runtime.Gosched()
    }
}

func (c *Channel) trySend(v float64) bool {
    if atomic.LoadUint32(&c.closed) == 1 {
        panic("can not send on a closed channel")
    }
    c.l.Lock()
    defer c.l.Unlock()
    if len(c.elements) == cap(c.elements) {
        return false
    }
    c.elements = append(c.elements, v)
    return true
}
```

---

## First, a baseline

```
$ time GOROUTINES=10 go run channel.go
total: +1.604671e+007
GOROUTINES=10 go run channel.go  2.98s user 0.10s system 184% cpu 1.668 total
```

---

```
$ time GOROUTINES=10 go run worst_channel.go
total: +1.604671e+007
done
GOROUTINES=10 go run worst_channel.go  16.38s user 0.08s system 365% cpu 4.498 total
```
## Oh...

Maybe if we add some `sync.Cond`?
```
$ time GOROUTINES=10 go run worse_channel.go
total: +1.604671e+007
done
GOROUTINES=10 go run worse_channel.go  21.03s user 0.11s system 368% cpu 5.744 total
```
## Ouch...
How about just plain `sync.Mutex` instead of the atomic lock?
```
$ time GOROUTINES=10 go run bad_channel.go
total: +1.604671e+007
done
GOROUTINES=10 go run bad_channel.go  14.09s user 0.11s system 275% cpu 5.149 total
```
## Yikes...

---

# Takeaway

 - Channels do a lot of work for you
 - Mutexes do a reasonable amount of work for you
 - Work comes at a cost

---

## When to use Mutexes

 - Serializing larger critical sections.
 - Granularity tradeoff
    - Overly broad locks:
        - Higher lock contention
        - Lower lock overhead
        - Lower risk
    - Overly tight locks:
        - Lower lock contention
        - Higher lock overhead
        - Higher risk

---

## When to use Channels

 - Thread-safe queue
 - Resource usage needs to be lower

---

## Final Thoughts

 - Synchronization is far from free and should be avoided if possible.
 - The more work you can do in parallel without communication the better.
 - x86 architectures were not designed for synchronization to be extremely fast.

## Recommendations

 - Use channels where they make sense, don't force it.
 - When in doubt use `sync.Mutex` and lean towards locking more broadly to prevent deadlocks and locking overhead.
 - Measure using `testing.B`. Don't make decisions in the dark. These things can often be counterintuitive.
 - Use atomics when you need performance or if you are writing code that naturally lends itself to using atomics.
 - Run your benchmarks on multiple cores w/ `GOMAXPROCS` set and run your test binaries on multiple architectures if you are targeting

---

class: center, middle

# Thanks! Feedback? Questions?

github.com/jasonkeene

---

class: center, middle

# Thanks! Feedback? Questions?

<span class="strike">github.com/jasonkeene</span><br>github.com/nelsam

</textarea>
<script src="https://gnab.github.io/remark/downloads/remark-latest.min.js">
</script>
<script>
remark.create()
</script>
</body>
</html>
